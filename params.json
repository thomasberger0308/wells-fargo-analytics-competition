{"name":"Wells-fargo-analytics-competition","tagline":"","body":"This page documents the result of the Wells Fargo competition \"Use social data to identify drivers of online conversations\". At this competition a data-set with 220,377 Twitter and Facebook messages was provided and has been analyzed as seen in the following steps.\r\n#Big Picture\r\nThe following picture gives an overview about data processing  steps\r\n![](http://i.imgur.com/Nl1vUOr.png)\r\n#Loading and Filtering\r\n##Load Data\r\n``` R\r\nlibrary(tm) \r\nlibrary(rpart)\r\n\r\n# Create data frame of dataset\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\n\r\n# change column 'FullText' to character\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\ndf$FullText = df.texts.clean$FullText\r\nrm(df.texts.clean)\r\n\r\n# change content of column 'Fulltext' to lowercase\r\ndf$FullText = tolower(df$FullText)\r\n```\r\n##Filter Banks\r\n``` R\r\n# Filter banks\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"banka\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"bankb\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"bankc\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"bankd\",x)))\r\n\r\n# Enter new column with bank name\r\n# initialize with 'No'\r\ndf$BankID = \"No\"\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\n# remove messages where BankA, BankB, BankC or BankD is not mentioned\r\ndf <- df[-which(df$BankID=='No'), ]\r\n```\r\n##Filter Topics\r\n``` R\r\n# find messages with the topic 'atm'\r\natm.idx = which(sapply(df$FullText,function(x) grepl(\"atm\",x)))\r\n\r\n# find messages with the topic 'online'\r\nonline1.idx = which(sapply(df$FullText,function(x) grepl(\"online\",x)))\r\nonline2.idx = which(sapply(df$FullText,function(x) grepl(\"app\",x)))\r\n\r\n# find messages with the topic 'investment'\r\ninvestment1.idx = which(sapply(df$FullText,function(x) grepl(\"investor\",x)))\r\ninvestment2.idx = which(sapply(df$FullText,function(x) grepl(\"investment\",x)))\r\n\r\n# find messages with the topic 'fraud'\r\nfraud.idx = which(sapply(df$FullText,function(x) grepl(\"fraud\",x)))\r\n\r\n# find messages with the topic 'service'\r\nservice.idx = which(sapply(df$FullText,function(x) grepl(\"service\",x)))\r\n\r\n# find messages with the topic 'fee'\r\nfee.idx = which(sapply(df$FullText,function(x) grepl(\"fee\",x)))\r\n\r\n# find messages with the topic 'job'\r\njob1.idx = which(sapply(df$FullText,function(x) grepl(\"interview\",x)))\r\njob2.idx = which(sapply(df$FullText,function(x) grepl(\"recruiting\",x)))\r\njob3.idx = which(sapply(df$FullText,function(x) grepl(\"recruitment\",x)))\r\njob4.idx = which(sapply(df$FullText,function(x) grepl(\"job\",x)))\r\n\r\n\r\n# Enter new column with topics\r\n\r\n# initialize with 'No'\r\ndf$Label = \"No\"\r\n\r\n```\r\n##RStudio Demo\r\n<embed width=\"420\" height=\"315\" src=\"http://www.youtube.com/watch?v=81EHUXZZkfE\">\r\n\r\n#Word Cloud\r\n##Source Code\r\n```R\r\n# since it is very slow otherwise, make the dataset smaller\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\n# create a corpus from data frame\r\ndocs <- Corpus(DataframeSource(as.data.frame(df.10000[,6]))) \r\n\r\n# Removing punctuation\r\ndocs <- tm_map(docs, removePunctuation)   \r\n# Removing numbers\r\ndocs <- tm_map(docs, removeNumbers) \r\n#Converting to lowercase\r\ndocs <- tm_map(docs, tolower)\r\n#Removing “stopwords” (common words) that usually have no analytic value.\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))   \r\n#Removing particular words\r\ndocs <- tm_map(docs, removeWords, c(\"banka\", \"bankb\",\"bankc\", \"bankd\", \"bank\", \r\n                                    \"twithndl\", \"twithndlbanka\",\"twithndlbanka\",\"twithndlbankb\",\r\n                                    \"twithndlbankc\",\"twithndlbankd\", \"name\",\"rettwit\",\"http\")) \r\n#Removing common word endings (e.g., “ing”, “es”, “s”)\r\nlibrary(SnowballC)   \r\ndocs <- tm_map(docs, stemDocument)  \r\n#Stripping unnecesary whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\n#Finish\r\ndocs <- tm_map(docs, PlainTextDocument) \r\n\r\n#create a document term matrix\r\ndtm <- DocumentTermMatrix(docs)\r\n#transpose of this matrix\r\ntdm <- TermDocumentMatrix(docs) \r\n\r\n#  Start by removing sparse terms:   \r\ndtms <- removeSparseTerms(dtm, 0.1)    \r\n\r\n# Organize terms by their frequency\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\n#  Start by removing sparse terms:   \r\ndtms <- removeSparseTerms(dtm, 0.1) \r\n\r\n#Word Cloud 100 most words\r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")   \r\nwordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)   \r\n```\r\n##Result\r\n![](http://i.imgur.com/B9HMQbG.png)\r\n\r\n\r\n#Sentiment\r\n##Source Code\r\n```R\r\n# Based on: http://www.ihub.co.ke/blogs/23216\r\n\r\n# Only need to do once\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n#system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\ndf.sentiment = df[]\r\n\r\nscores = score.sentiment(df.sentiment$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n\r\nscores$bankid = df.sentiment$BankID\r\n\r\n# colors\r\ncols = c(\"#CC0000\", \"#99FF00\", \"#FFCC00\", \"#3333FF\")\r\nnames(cols) = c(\"BankA\", \"BankB\", \"BankC\", \"BankD\")\r\n\r\n# boxplot\r\nlibrary(ggplot2)\r\n#Bank's Sentiment Scores\r\nggplot(scores, aes(x=bankid, y=score, group=bankid)) +\r\n  geom_boxplot(aes(fill=bankid)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Bank's Sentiment Scores\") + \r\n  xlab('Bank') + ylab('Sentiment Score')\r\n\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$bankid, mean)\r\ndf.plot = data.frame(bankid=names(meanscore), meanscore=meanscore)\r\ndf.plot$bankids <- reorder(df.plot$bankid, df.plot$meanscore)\r\n\r\nggplot(df.plot, aes(x = factor(bankids), y = meanscore, fill=bankids)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score\") + \r\n  xlab('Bank') + ylab('Average Score')\r\n\r\n# barplot of average very positive\r\nbankid_pos = ddply(scores, .(bankid), summarise, mean_pos=mean(very.pos))\r\nbankid_pos$bankids <- reorder(bankid_pos$bankid, bankid_pos$mean_pos)\r\n\r\nggplot(bankid_pos, aes(x = factor(bankid), y = mean_pos, fill=bankid)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score\") + \r\n  xlab('Bank') + ylab('Average Score')\r\n\r\n# barplot of average very negative\r\nbankid_neg = ddply(scores, .(bankid), summarise, mean_neg=mean(very.neg))\r\nbankid_neg$bankids <- reorder(bankid_neg$bankid, bankid_neg$mean_neg)\r\n\r\nggplot(bankid_neg, aes(x = factor(bankid), y = mean_neg, fill=bankid)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score\") + \r\n  xlab('Bank') + ylab('Average Score')\r\n\r\n```\r\n##Results\r\n**Output for Banks:**\r\n![](http://i.imgur.com/88tnRYk.png)\r\n![](http://imgur.com/4DLQcqg)\r\n![](http://imgur.com/neQZAm6)\r\n![](http://imgur.com/59vBLVZ)\r\n**Output for Topics: (slightly modified R code)**\r\n![](http://i.imgur.com/ZrbEP99.png)\r\n![](http://i.imgur.com/daQHZfT.png)\r\n![](http://i.imgur.com/daQHZfT.png)\r\n![](http://i.imgur.com/QcbYGda.png)\r\n**Output Topics Average by Bank: (slightly modified R code)**\r\n\r\n#Report\r\n\r\nThere truly is no such thing as perfection especially in the banking world. There is always a complaint, an employee error, a problem with something. In today’s day and age, the use of online banking is growing at a stupendous rate and with the emergence of online banking, there is also online feedback twenty-four-seven. The alarming growth of online banking leads to both outstanding achievements of service and productivity, and unfortunately a long list of dissatisfaction ranging from crashed servers to online problems. Naturally the effects of technological advances are either positive or detrimental to the consumer’s outlook on the bank as a whole. The emotionally connections created between the consumer and the bank can either blossom or fall faster than the leaves in autumn. In terms of the plots shown below, sentiment means emotional reaction towards that bank. There are four banks titled, starting from “Bank A” through “Bank D.” Below are visual representations of data that was sifted out of thousands of twitter comments in regard to each bank: \r\n![](http://i.imgur.com/VlSKM57.png)\r\nThis diagram shown above perfectly brings the term “sentiment” to life. As one can see, there is a possibility of a zero sentiment level. Also, there is both positive or negative. “Fraud” is a low sentiment rating on average for every bank, meaning the word “fraud” has the highest percent of average distaste in all the banks. Because this diagram is based off of averages in all the banks, the readings are most valuable when looking at the whole banking field, thus comparing specific bank’s readings compared to the average of all four.\r\n\r\nEach respective bank had their own sentiment score which played a factor in the topic sentiment score. That being said, the sentiment score of the banks are all the sentiment scores of each word being compiled into each individual bank and then scored.\r\n![](http://i.imgur.com/1w0arTA.png)\r\nTopics are difficult to determine in a concrete way, meaning that the topics that are shown below are abstract and some tweets didn’t fall into any of these categories.\r\n![](http://i.imgur.com/5EXzCOU.png)\r\nLooking at the figure above, it is clear to see a trend in what topic occurs most often and least often on average for all of the banks. “Online,” is clearly the most used topic when labelling tweets took place. What the topic, “online,” means is that the content of the tweet is referring to online resources for the banks; whereas the topic, “ATM,” refers to tweets exclusively regarding ATM complaints or praises on average in all the banks.\r\n\r\nEach bank had their own trends and the most important trend was that of sentiment. The trend of sentiment being used in determining the consumer’s acceptance or the opposite when referring to emotional reactions towards the respective banks.\r\n![](http://i.imgur.com/jlU23Cb.png)\r\nBank A, as a whole, had the highest sentiment score, meaning that Bank A had the highest consumer satisfaction.\r\n\r\nThe evidence presented in the form of graphs and plots, is a direct representation of the information that was received via Twitter and each respective plot or graph is determined over the scale of about two hundred thousand tweets. Among the numerous plots and graphs that were created, the four shown above were the most informative and frankly were the most important in terms of visual representation of a group of banks. In these four visual diagrams, there was over two hundred thousand twitter entries regarding at least one of these banks. Surely that is a sample size so small that in a few years that number will have grown exponentially to the point where entries are coming in twenty-four-seven. \r\n\r\nWith the rise of technology and the use of online banking, the measurement of consumer satisfaction would not be as readily available. Twitter gives the online banking community feedback directly following an incident or situation. This boom of social media has helped the banking community, which is the main piece that online banking is a part of, grow and change their policies, employees, and strategies in the hopes of being as close to a perfect bank as possible.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}